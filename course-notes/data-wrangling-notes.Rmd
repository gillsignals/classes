---
title: "Data Wrangling - HarvardX Class"
author: "Amy Gill"
date: "October 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Topics

-importing data into R
-tidying data
-string processing
-html parsing
-working with dates and times

```{r libraries}
library(dslabs)
library(readr)
library(readxl)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(rvest)
library(htmlwidgets)
library(purrr)
```

## Paths and the Working Directory

Change working directory with *setwd()*, check it with *getwd()*.

```{r }
getwd()
```

Unless a full path is specified, R will look at your working directory.

Files from the *dslabs* package are in:
```{r}
path = system.file("extdata", package = "dslabs")
list.files(path)
```

Merge file path parts with *file.path()*. Copy with *file.copy()*. Check if *file.exists()*.

```{r}
filename = "murders.csv"
fullpath <- file.path(path, filename)
file.copy(fullpath, file.path(getwd(), "data"))  #  note this returns FALSE if the file already exists
file.exists(file.path("data", filename))  # checks in working directory for data/filename
```

The *readr* package and *readxl* package expand base R functionality by providing consistent syntax commands with which to read delimited spreadsheets.

```{r}
times <- read_csv("data/times.txt")
times_2016 <- read_excel("data/times.xlsx", sheet = 2)
times_2016_2 <- read_excel("data/times.xlsx", sheet = "2016")
identical(times_2016, times_2016_2)
```

Base R provides import functions as well - *read.table()*, *read.csv()*, *read.delim()*.
Importantly, characters will be converted to factors by default.
Note also that the class of the objects will simply be data.frame, not tibble.

read_csv etc can take URLs directly.
If you want to save a local version of a file, use *download.file()*.

```{r}
url <- "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
download.file(url, "data/murders.csv")
murders <- read_csv(url)
```

Two useful functions when downloading files from the internet are:
*tempdir()* - make a temporary directory with a name likely to be unique
*tempfile()* - make a temporary file with a name likely to be unique

You can then remove temporary directories and files with *file.remove()*.
```{r}
tmp_filename <- tempfile()
download.file(url, tmp_filename)
dat <- read.csv(tmp_filename)
file.remove(tmp_filename)
head(dat)
```
## Tidy Data

```{r}
data("gapminder")
tidy_data <- gapminder %>%
  filter(country %in% c("South Korea", "Germany")) %>%
  select(country, year, fertility)
head(tidy_data)
```

With the data in tidy format, we can easily plot.
```{r}
tidy_data %>%
  ggplot(aes(year, fertility, color = country)) +
  geom_point()
```
Tidy data =
Each row is an observation
Each column is a different variable

Most data is originally not in a tidy format.
The original fertility data can be seen here:
```{r}
path <- system.file("extdata", package = "dslabs")
filename <- file.path(path, "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
```

The same data are present, but they are in a wide format - column names are variables, and several observations are present in each row.

```{r}
select(wide_data, country, '1960':'1967')
```

To properly graph this data with tidyverse tools, we need to wrangle this data into tidy format.

The *tidyr* package

*gather()* combines columns specified.
*gather(new_variable, new_values, source_columns)*
arg1 = name of new column to hold variable currently in wide columns
arg2 = name of new column to hold values
arg3 = columns to gather OR -(columns not to gather)
```{r}
new_tidy_data <- wide_data %>%
  gather(year, fertility, '1960':'2015')
head(new_tidy_data)
```

Note *gather()* assumes column names are characters.
To avoid this, use the argument *convert*.
```{r}
class(tidy_data$year)
class(new_tidy_data$year)

new_tidy_data <- wide_data %>%
  gather(year, fertility, -country, convert = TRUE)
class(new_tidy_data$year)
```
Now that data is tidy, we can effectively use ggplot commands.
```{r}
new_tidy_data %>%
  ggplot(aes(year, fertility, color = country)) +
  geom_point()
```

It is often useful to convert tidy data to wide with the *spread()* function as an intermediate step in wrangling.
*spread(colnames, values)*
arg1 = variable to use as column names
arg2 = variable with values to use to fill cells
```{r}
new_wide_data <- new_tidy_data %>% spread(year, fertility)
select(new_wide_data, country, '1960':'1967')
```

To summarize reshaping data - use *gather()* and *spread()* to reorganize values of a table into a new layout.

*gather(data, key, value, ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE)* moves columns [...] into a key column, gathering values into a single value column

*spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE, sep = NULL)* moves unique values of a key column into column names, spreading values of a value column across new columns
```{r}
d <- read_csv("data/times.csv")
d
```
To tidy the data, gather columns 2015 through 2017:
```{r}
tidy_data <- d %>%
  gather(year, time, '2015':'2017')
tidy_data
```
To convert data back to the wide format, spread year and time:
```{r}
tidy_data %>% spread(year, time)
```

Often data wrangling is much more demanding. Here is a slightly more realistic example.
```{r}
#  File path provided by Rafa
path <- system.file("extdata", package = "dslabs")
filename <- file.path(path, "life-expectancy-and-fertility-two-countries-example.csv")

#  Save a local copy to data directory
file.copy(filename, file.path(getwd(), "data"))

raw_dat <- read_csv(filename)
select(raw_dat, 1:5)
```
This table is in wide format. Columns include year. The variables of fertility and life expectancy are also in the column names.

We can start the data wrangling with *gather()*.
```{r}
dat <- raw_dat %>% gather(key, value, -country)
head(dat)
```

Encoding two variables in a column name is a common problem with tools available to address it.

Our key has two variables merged in one. We want to split these into two separate columns. We can do this with the *separate()* function.

arg1 = data
arg2 = column to separate
arg3 = names for new columns
arg4 = delimiter

```{r}
dat %>% separate(key, c("year", "variable_name"), "_")
```
Note that the underscore is the default separator.

BUT see that we received a warning about having too many pieces and that the underscore truncated "life_expectancy" to "life".

This occurs because we didn't only have underscores between year and variable, but also in some cases in between components of variable name.

We can choose to split on all underscores and fill in any rows that are missing a variable with NAs:
```{r}
dat %>%
  separate(key, c("year", "first_variable_name", "second_variable_name"), fill = "right")
```
In some cases, this is an appropriate way to handle splitting.

But if we read the separate file, we find a better approach is to merge the last two variables using the argument extra, like this:
```{r}
dat %>%
  separate(key, c("year", "variable_name"), sep = "_", extra = "merge")
```

If the separate columns already exist, they can be joined with *unite()*.

```{r}
dat %>%
  separate(key, c("year", "first_variable_name", "second_variable_name"), fill = "right") %>%
  unite(variable_name, first_variable_name, second_variable_name, sep="_")
```


Now, we can make the data tidy by creating a column for each variable using *spread()*.
```{r}
dat %>%
  separate(key, c("year", "variable_name"), sep = "_", extra = "merge") %>%
  spread(variable_name, value)
```

Let's tidy data from two years of average race times.
```{r}
d <- read_csv("data/times2.csv")
d
```
```{r}
tidy_data <- d %>%
  gather(key = "key", value = "value", -age_group) %>%
  separate(key, c("year", "variable_name"), sep = "_") %>%
  spread(key = variable_name, value = value)
tidy_data
```

## Combining Tables

The information we need for a given analysis may not be in just one table.

Suppose we want to explore the relationship between the population of states, which we have in this table:
```{r}
data(murders)
head(murders)
```
And electoral votes, which we have in this one:
```{r}
data(polls_us_election_2016)
head(polls_us_election_2016)
```
Note that we can't simply align the tables side by side because the order of states is not the same:
```{r}
identical(results_us_election_2016$state, murders$state)
```
Join functions based on SQL joins are useful in these cases.
Identify one or more columns with the information needed to match the two tables.
```{r}
tab <- left_join(murders, results_us_election_2016, by = "state")
head(tab)
```
```{r}
tab %>%
  ggplot(aes(population/10^6, electoral_votes, label = abb)) +
  geom_point() +
  geom_text_repel() +
  scale_x_continuous(trans = "log2") +
  scale_y_continuous(trans = "log2") +
  geom_smooth(method = "lm", se = FALSE)
```

There is a roughly linear relationship between population and electoral votes, with roughly 2 electoral votes per million people, but with smaller states getting a higher ratio.

In practice, not every row in one table has a matching row in another. So, there are several ways to join.

Let's demonstrate with some subsets of our data.
```{r}
tab1 <- slice(murders, 1:6) %>%
  select(state, population)
tab1
```
```{r}
tab2 <- slice(results_us_election_2016, c(1:3, 5, 7:8)) %>%
  select(state, electoral_votes)
tab2
```
*left_join()* completes all rows that are in the table from argument 1, supplementing missing data with NA. We get the table from argument 1, with appended columns from the additional table:
```{r}
left_join(tab1, tab2)
```
Note that joins can also be attached to pipes.

*right_join()* completes the table with all data from argument 2, then fills columns by merging data from matching rows in argument 1, supplementing data with NA:
```{r}
tab1 %>% right_join(tab2)
```
To keep only the rows that have information in both tables, use *inner_join()* - this acts as an intersect operation:
```{r}
tab1 %>% inner_join(tab2)
```

To keep all rows from all tables and supplement all missing info with NA, use *full_join()* - this is equivalent to a union operation:
```{r}
tab1 %>% full_join(tab2)
```
Two other joins let you keep parts of one table depending on what's in the other.

*semi_join()* keeps rows from the first table from which we have information in the second. It does not add any info from the second.
```{r}
tab1 %>% semi_join(tab2)
```
*anti_join()* is the opposite - it keeps rows from the first table for which there is no information in the second.
```{r}
tab1 %>% anti_join(tab2)
```
Another common way to combine data sets is by binding.
Binding does no check by row/col content for matching.
*bind_cols()* binds columns of same dimension into tibble.
Note that the base R function *cbind()* does not make a tibble.
```{r}
bind_cols(a=1:3, b=4:6)
```
*bind_cols()* can also bind data.frames:
```{r}
tab1 <- tab[, 1:3]
tab2 <- tab[, 4:6]
tab3 <- tab[, 7:9]
new_tab <- bind_cols(tab1, tab2, tab3)
head(new_tab)
```
*bind_rows()* is similar but binds rows instead of columns, and replaces the base R function *rbind()*.

Set operations like *union()*, *intersect()*, and *setdiff()* can act on vectors.
*union(df1, df2)* returns rows in either df1 or df2.
*intersect(df1, df2)* returns rows in both df1 and df2.
*setdiff(df1, df2)* returns rows in df1 but not in df2.

If *dplyr* is loaded, set operations also work on tibbles and data frames. *setequal()* can determine whether two sets contain the same values regardless of order, and returns which rows in each set are not shared.

```{r}
x = c("a", "b")
y = c("a", "a")

setdiff(x, y)
setdiff(y, x)

```
## Web Scraping with the rvest Package

Data isn't always in a spreadsheet ready to read. For example, the US Murders data used in earlier courses came from this Wikipedia page:

https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state

To get these data, we have to do web scraping.

This is possible because information used by a browser to render web pages is received as text from a server in HTML - hypertext markup language.

In Chrome, right click and hit view source. Because we can see the HTML, we can pull it into R and extract the information we need from the page.

If you know HTML, you are aware of the patterns used in the language to store data.

We can also make use of a language widely used to make webpages look pretty, cascading style sheets (CSS).

Although there are tools for scraping that are useful in the absence of HTML, it is quite useful to learn some HTML and some CSS. Links are available in the course material.

The first step is to import the webpage into R using code. The *rvest* package helps import html documents.
```{r}
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
h <- read_html(url)
class(h)
```

The *harvest* package handles XML documents too.

Now that we have the object h, how do we extract a table from the object?
We can surf the code to see that it's in an html table

Different parts of HTML documents often define messages between angle brackets, or nodes.

*rvest* includes functions to extract nodes from HTML documents.

*html_node()* extracts the first node of a type.

*html_nodes()* extracts all nodes of that type.

```{r}
tab <- h %>% html_nodes("table")
tab <- tab[[2]]
tab
```
This is clearly not a tidy data set - it's not even a data frame.

Use *html_table()* to convert an html table to a data frame.
```{r}
tab <- tab %>% html_table
class(tab)
```
Let's change the column names, which are a bit long:
```{r}
tab <- tab %>% setNames(c("state", "population", "total", "murders", "gun_murders", "gun_ownership", "total_rate", "murder_rate","gun_murder_rate"))
head(tab)
```

Some of the columns that are supposed to be numbers are actually characters, and to make things worse some of them have commas which disrupts converting them to numbers.

Before we practice cleaning data more, let's discuss general approaches to extracting information from websites.

## CSS Selectors

The default look of webpages made with basic HTML is quite unattracive. Aesthetically pleasing pages are made using CSS to add style to webpages.

In general, CSS files work by defining how each of the elements of a webpage will look. Title, headings, itemized lists, tables, links, etc., each receive their own style including font, color, size, distance from margin, etc.

To do this, CSS leverages patterns to define these elements known as *selectors*. If we know we want to grab data from a webpage and know a selector unique to that part of the page, we can use *html_nodes*.

However, knowing which selector to use can be quite complicated. Say we want to extract the recipe name, total preparation time, and list of ingredients from a guacamole recipe here: http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609

This looks incredibly difficult, but selector gadgets make this possible. SelectorGadget at selectorgadget.com allows you to interactively determine which CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables, it is highly recommended. A Chrome extension is available to permit you to turn on the gadget highlighting parts of the page as you click through, showing the necessary selector to extract those segments.

The dirty work has already been done for the following selectors on this guacamole recipe:
```{r}
h <- read_html("http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")

recipe <- h %>%
  html_node(".o-AssetTitle__a-HeadlineText") %>%
  html_text()

prep_time <- h %>%
  html_node(".o-RecipeInfo__a-Description--Total") %>%
  html_text()

ingredients <- h %>%
  html_nodes(".o-Ingredients__a-ListItemText") %>%
  html_text()
```
We can now extract what we want and create a list.
```{r}
guacamole <- list(recipe, prep_time, ingredients)
guacamole
```

The recipe pages from this website follow a general layout, so we can use this code to create a function that extracts this 
information:

```{r}
# The get_recipe function scrapes recipe name, prep time, and ingredients from a Food Network webpage
get_recipe <- function(url) {
  
  # Read in url
  h <- read_html(url)
  
  # Extract recipe name
  recipe <- h %>%
    html_node(".o-AssetTitle__a-HeadlineText") %>%
    html_text()
  
  # Extract prep time
  prep_time <- h %>%
    html_node(".o-RecipeInfo__a-Description--Total") %>%
    html_text()
  
  # Extract ingredient list
  ingredients <- h %>%
    html_nodes(".o-Ingredients__a-ListItemText") %>%
    html_text()
  
  # Return a list of recipe name, prep time, ingredient list
  return(list(recipe = recipe, prep_time = prep_time, ingredients = ingredients))
}
```

The get_recipe function can now be used on an any of their webpages.

```{r}
get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```

Several other powerful tools provided by *rvest* are not covered here. For example, you can query a webpage from R with *html_form()*, *set_values()*, and *submit_form()*. This is a more advanced topic that can be explored at a future point.

## String Processing

Allows you to
-remove unwanted characters
-extract numeric values
-find and replace characters
-extract specific parts of strings
-convert free form text into more uniform formats
-split strings into multiple values
-use regular expressions (regex) to process strings

Let's revisit the raw murders data:
```{r}
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table()
murders_raw <- murders_raw[[2]] %>%
  setNames(c("state", "population", "total", "murders", "gun_murders", "gun_ownership", "total_rate", "murder_rate", "gun_murder_rate"))
head(murders_raw)
```
Two of the columns we expected to contain numbers actually contain characters. This is common because web documents often include commas in numbers to improve readability.
```{r}
class(murders_raw$population)
class(murders_raw$total)
```
Because commas in numbers are so common, the *parse_number()* function is prewritten to handle this conversion. Many other string processing challenges are custom and will need to be addressed.

To define your string, use single or double quotes. You need to be picky if your string includes the quotes described. Note that backquotes are not an option. Also note that you can see the string using *cat()*.
```{r}
s <- '10"'
cat(s)
```
```{r}
t <- "5'"
cat(t)
```

To escape a quote, use the backslash \. This is commonly required when processing strings.
```{r}
s <- "5'10\""
cat(s)
```
```{r}
t <- '5\'10"'
cat(t)
```

The *stringr* package helps process strings. Base R includes functions for all of these tasks, but they don't follow common conventions and syntax. *stringr* is easy to use, especially because its functions all start with str_ and can be found with autocomplete and because their first arguments are always the input string so they can be combined with pipes.

*str_detect()* helps us detect whether a certain string or character is present.

```{r}
commas <- function(x) any(str_detect(x, ","))
murders_raw %>% summarize_all(funs(commas))
```

We can then replace all the commas with *str_replace_all()*.
```{r}
test_1 <- str_replace_all(murders_raw$population,",", "")
test_1 <- as.numeric(test_1)
test_1
```
Since this function is so common, it is pre-written as *parse_number()*.
```{r}
test_2 <- parse_number(murders_raw$population)
identical(test_1, test_2)
```
We want to apply this function to both the population and total columns (columns 2 and 3), so we can apply *mutate_at()*.
```{r}
murders_new <- murders_raw %>%
  mutate_at(2:3, parse_number)
murders_new %>% head
```
Let's observe raw reported heights from students.
```{r}
data(reported_heights)
```
Instructions asked for height in inches, but clearly not everyone complied in the web form.
```{r}
class(reported_heights$height)
```
Let's see what kinds of height entries gave us errors.
```{r}
reported_heights %>%
  mutate(new_height = as.numeric(height)) %>%
  filter(is.na(new_height)) %>%
  head(n=10)
```
Some students did not report height in inches as requested. Some included punctuation, others reported in cm, still others gave nonsense values (it's over 9000!). Good job, college students.

We could discard these; however, many of these follow patterns that, in principle, we could easily convert to inches.

We see various cases of x' y" where x=feet and y=inches.

We could correct these cases by hand. However, humans are prone to making mistakes. Also, since we plan to keep collecting data going forward, it will be convenient to write code to do this automatically.

First, survey the problematic entries and try to define specific patterns followed by large groups of entries. The larger the groups, the more entries we can fix with a single programmatic approach.

Let's try to identify a pattern that can be accurately identified with a rule, such as a digit followed by a feet symbol, followed by 1-2 digits followed by an inches symbol.

We write code to filter out the normal output and only give us the problematic entries. We also use *suppressWarnings()* to avoid the warning messages we expect will be thrown by *as.numeric()*.
```{r}
# The function not_inches returns the indices of entries for which values are NA or unreasonably short/tall for our class distribution in inches
not_inches <- function(x, smallest = 50, tallest = 84) {
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest
  ind
}
```

Now we can apply this function and find the number of problematic entries.
```{r}
problems <- reported_heights %>%
  filter(not_inches(height)) %>%
  .$height
length(problems)
```
Let's examine the entries to determine the patterns they share.
```{r}
problems
```
Pattern 1: x' y or x' y" or x' y\" with x=feet and y=inches
```{r}
# Regexs will be explained below
pattern1 <- "^\\d\\s*'\\s*\\d{1,2}\\.*\\d*'*\"*$"
str_subset(problems, pattern1) %>% head(n=10) %>% cat
```

Pattern 2: x.y or x,y with x=feet and y=inches
```{r}
pattern2 <- "^[4-6]\\s*[\\.|,]\\s*([0-9]|10|11)$"
str_subset(problems, pattern2) %>% head(n=10) %>% cat
```
Pattern 3: Entries in centimeters rather than inches
```{r}
ind <- which(between(suppressWarnings(as.numeric(problems))/2.54, 54, 81))
ind <- ind[!is.na(ind)]
problems[ind] %>% head(n=10) %>% cat
```
## Regular Expressions

Regular expressions (regex) allow us to code patterns into a standardizd format that can be recognized and converted automatically.

We will use regex on the patterns we identified earlier. After applying these steps, we will then check again to see what entries were not fixed, and see if we can tweak our approach to be more comprehensive.

These interactive approaches are very common in data science.

At the end, we hope to have a script that makes web-based data collection robust to the most common user mistakes.
```{r}
str_detect(murders_raw$total, ",")
```
```{r}
str_subset(reported_heights$height, "cm")
```

Let's consider a slightly more complicated example and ask which of the following strings satisfy your pattern.
```{r}
yes <- c("180 cm", "70 inches")
no <- c("180", "70 ''")
s <- c(yes, no)
```
So, we're asking which strings contain the pattern cm or the pattern inches.

We could call *str_detect()* twice:
```{r}
str_detect(s, "cm") | str_detect(s, "inches")
```
However, we don't need to do this - we can use regex instead.

In this case, we just want to nest the OR operator | in a regex:
```{r}
str_detect(s, "cm|inches")
```
*\d* means any digit [0-9]. In R, we have to escape the backslash, so we actually have to use two backslashes to represent digits as *\\d*.
```{r}
yes <- c("5", "6", "5'10", "5 feet", "4'11")
no <- c("", ".", "Five", "six")
s <- c(yes, no)
pattern <- "\\d"
str_detect(s, pattern)
```
We can observe the strings that satisfy the regex and where with *str_view()*, which shows the first match for each string. 
```{r}
str_view(s, pattern)
```
*str_view_all()* shows all matches.
```{r}
str_view_all(s, pattern)
```

We create strings to test our regexs - some that we know should match and some that we know should not. This permits us to check for the two types of errors - failing to match and matching inappropriately.

Character classes are defined inside square brackets

[56] = 5 or 6
```{r}
str_view(s, "[56]")
```
Character classes are compatible with ranges. But recall that in regexs everything is a character, not a number, so a range like 1-20 is not the numerical range 1-20 and 0-9 is the largest range.

[4-7] = 4, 5, 6, 7
[0-9] == \\d
[1-20] = 1, 2 then the character 0
```{r}
yes <- as.character(4:7)
no <- as.character(1:3)
s <- c(yes, no)
str_detect(s, "[4-7]")
```
Alphabetical characters have ranges too.

[a-z] lower case letters
[A-Z] upper case letters
[a-zA-Z] all letters

Anchors indicate strings must start or end at specific places.

^ beginning of string
$ end of string

^\\d$ start of the string, one digit, end of the string
```{r}
pattern <- "^\\d$"
yes <- c("1", "5", "9")
no <- c("12", "123", " 1", "a4", "b")
s <- c(yes, no)
str_view(s, pattern)
```
Quantifiers in curly braces specify how many of a pattern are acceptable to satisfy the query. The asterisk is a special quantifier meaning zero or more instances.

\\d{1,2} = 1 or 2 digits
\\d* = zero or more digits
```{r}
pattern <- "^\\d{1,2}$"
yes <- c("1", "5", "9", "12")
no <- c("123", " 1", "a4", "b")
s <- c(yes, no)
str_view(s, pattern)
```
We now know everything we need to detect strings in our height input with common errors.

pattern 1:
-start of the string
-number 4 to 7
-foot symbol
-one or two digits
-inch symbol
end of string
```{r}
pattern <- "^[4-7]'\\d{1,2}\"$"
yes <- c("5'7\"", "6'2\"", "5'11\"")
no <- c("6,2\"", "6.2\"", "I am 5'11\"", "3'2\"", "64")
s <- c(yes, no)
str_detect(s, pattern)
```
Only a few of our problematic entries have this pattern.
```{r}
sum(str_detect(problems, pattern))
```
We show examples here that expose why we don't have more matches.
```{r}
problems[c(2, 10, 11, 12, 15)] %>% str_view(pattern)
```
Some students wrote out feet and inches. we can see which entries with *str_subset()*
```{r}
str_subset(problems, "inches")
```
Some entries used single quotes twice to respresent inches instead of double quotes.
```{r}
str_subset(problems, "''")
```
We will address this problem by first representing feet and inches in a uniform way.

Use single quote for feet and nothing for inches so we don't have to escape it. x'y will represent x feet y inches.

We can adjust our pattern by removing the inch sign. We use *str_replace()* to change various foot markings to ' and remove various inch markings. 
```{r}
pattern <- "^[4-7]'\\d{1,2}$"

problems %>%
  str_replace("feet|ft|foot", "'") %>%
  str_replace("inches|in|''|\"","") %>%
  str_detect(pattern) %>%
  sum
```
Whitespace is not ignored - 5' 4\" would not be recognized by our pattern because there is a space after the '.
```{r}
identical("hi", "hi ")
```
In regex, we can represent whitespace with *\\s*

*\\s* = space

We also use the asterisk quantifier to represent zero or more instances of the previous character.
```{r}
yes <- c("AB", "A1B", "A11B","A111B", "A1111B")
no <- c("A2B", "A21B")
s <- c(yes, no)
str_detect(s, "A1*B")
```
* = zero or more
? = zero or one
+ = one or more
```{r}
data.frame(string = yes,
           none_or_more = str_detect(yes, "A1*B"),
           none_or_once = str_detect(yes, "A1?B"),
           once_or_more = str_detect(yes, "A1+B"))
```
We can add the space options before and after the feet symbol and get a few more entries.
```{r}
pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"

problems %>%
  str_replace("feet|ft|foot", "'") %>%
  str_replace("inches|in|''|\"","") %>%
  str_detect(pattern) %>%
  sum
```
We might consider replacing all the spaces with empty strings with *str_replace_all()*, but when using this be careful it does not have unintended effects.

In this case, some entries are in the format "x y" where the space separates feet and inches, and replacing this whitespace will cause us to lose data.

Let's experiment with regexs modifying some university names.
```{r}
schools <- c("U. Kentucky", "Univ New Hampshire", "Univ. of Massachusetts", "University Georgia", "U California", "California State University")

schools %>%
  str_replace("^Univ\\.?\\s|^U\\.?\\s","University ") %>%
  str_replace("^University of |^University ", "University of ")
```
The second group of problematic entries were of the form x.y or x,y or x y. We want to change all of these to x'y.

Groups are a powerful aspect of regex that permits the extraction of values.

Groups are defined using parentheses. Encapsulate the part of the pattern that matches the parts to extract.
```{r}
pattern_without_groups <- "^[4-7],\\d*$"
```
We want to extract the digits so we can form the new version in x'y format.
```{r}
pattern_with_groups <- "^([4-7]),(\\d*)$"
```
The parentheses do not change the matching procedure, but only indicate that we want to save what is inside.
```{r}
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1", "5.9")
s <- c(yes, no)
str_detect(s, pattern_without_groups)
str_detect(s, pattern_with_groups)
```
We can extract the values with *str_match()*.
```{r}
str_match(s, pattern_with_groups)
```
First column is original pattern that was matched, then the two chunks we asked it to recognize and extract.

The special character for the ith extracted group is *\\i*. 

*\\1* - value extracted from first group
*\\2* - value extracted from second group

This code will replace a comma with an apostrophe, but only if it is between two digits.
```{r}
pattern_with_groups <- "^([4-7]),(\\d*)$"
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1", "5.9")
s <- c(yes, no)

str_replace(s, pattern_with_groups, "\\1'\\2")
```
We can now adapt our pattern to be a bit more flexible to catch all cases of x,y x.y x y format.
-start of string
-one digit 4-7 to extract
-zero or more spaces
-one or more commas, periods, or spaces
-zero or more spaces
-zero or more digits to extract
-end of string

```{r}
pattern_with_groups <- "^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$"
str_subset(problems, pattern_with_groups) %>%
  str_replace(pattern_with_groups, "\\1'\\2") %>%
  head
```

Now we can test our approach, search how well it worked, and tweak for possible improvements.

Let's write a function to detect everything that can't be converted to inches, remembering that several can be converted from cm.
```{r}
not_inches_or_cm <- function(x, smallest = 50, tallest = 84) {
  inches <- suppressWarnings(as.numeric(x))
  ind <- !is.na(inches) &
    ((inches >= smallest & inches <= tallest) |
       (inches/2.54 >= smallest & inches/2.54 <= tallest))
  !ind
}

problems <- reported_heights %>%
  filter(not_inches_or_cm(height)) %>%
  .$height
length(problems)
```
Let's see how many of these we can make fit our pattern after the processing steps we have developed.
```{r}
converted <- problems %>%
  str_replace("feet|foot|ft", "'") %>%  # convert feet symbols to '
  str_replace("inches|in|''|\"","") %>% # remove inches symbols
  str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2") # change to x'y format

pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
index <- str_detect(converted, pattern)
mean(index)
sum(index)
```
Let's examine the remaining cases that we are not matching.
```{r}
converted[!index]
```
-Some students measuring 5' or 6' didn't match our pattern.
-Some students measuring exactly 5 or 6 feet entered just that number without inches.
-Some inches were entered with decimal points.
-Some entries have spacecs at the end
-Some are in meters and some use European decimals (1,7 = 1.7 meter)
-Some added cm
-Some spelled out numbers

Some of these cases may be rare enough to be not worth fixing, but they are opportunities to practice.
```{r}
yes <- c("5 feet 7inches", "5 7")
no <- c("5ft 9 inches", "5 ft 9 inches")
s <- c(yes, no)

converted <- s %>%
  str_replace("feet|foot|ft", "'") %>%
  str_replace("inches|in|''|\"","") %>%
  str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2")

pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
str_detect(converted, pattern)
```
Case 1: Some students measuring exactly 5' or 6' didn't add '0. If we add *'0* then our pattern will match.
```{r}
yes <- c("5", "6", "5")
no <- c("5'", "5''", "5'4")
s <- c(yes, no)
str_replace(s, "^([4-7])$", "\\1'0")
```

The pattern says it has to start, be followed with a digit 4-7, then end. We replace it with just the digit extracted and follow with '0.

Case 2 covers the entries 5' and 6'. Case 4 covers 5 and 6. We can cover both by extracting cases where 5 or 6 is followed by zero or one foot symbols.
```{r}
str_replace(s, "^([56])'?$", "\\1'0")
```
Case 3 covers when inches include decimals. We must permit zero or one period *.* - period is a special character that means any character except a line break and must be escaped - followed by zero or more digits *\\d* 
```{r}
pattern <- "^[4-7]\\s*'\\s*(\\d+\\.?\\d*)$"
```
Case 5 is when meters use commas. We can approach similarly to how we converted x.y to z'y, requiring that the first digit is 1 or 2:
```{r}
yes <- c("1,7", "1, 8", "2, ")
no <- c("5,8", "5,3,2", "1.7")
s <- c(yes, no)
str_replace(s, "^([12])\\s*,\\s*(\\d*)$", "\\1\\.\\2")
```
*str_trim()* removes whitespace from the start and end of a string.
```{r}
str_trim("5 ' 5 ")
```
Capitalize with *str_to_upper()* and lowercase with *str_to_lower()*. We can combine this with a function to convert words to numbers to handle cases where students wrote numbers as words instead of digits.
```{r}
words_to_numbers <- function(s) {
  str_to_lower(s) %>%
    str_replace_all("zero", "0") %>%
    str_replace_all("one", "1") %>%
    str_replace_all("two", "2") %>%
    str_replace_all("three", "3") %>%
    str_replace_all("four", "4") %>%
    str_replace_all("five", "5") %>%
    str_replace_all("six", "6") %>%
    str_replace_all("seven", "7") %>%
    str_replace_all("eight", "8") %>%
    str_replace_all("nine", "9") %>%
    str_replace_all("ten", "10") %>%
    str_replace_all("eleven", "11")
}
```
Let's create a function using all the operations we wrote to clean our data:
```{r}
convert_format <- function(s) {
  s %>%
    str_replace("feet|foot|ft", "'") %>% # convert feet symbols to '
    str_replace_all("inches|in|''|\"|cm|and","") %>% # remove inches and other symbols
    str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2") %>% # change x,y x.y x y to x'y
    str_replace("^([56])'?$", "\\1'0") %>% # add '0 to 5 or 6
    str_replace("^([12])\\s*,\\s*(\\d*)$", "\\1\\.\\2") %>% # change European decimal
    str_trim() # remove whitespace
}
```

Now we can see which problematic entries are left.
```{r}
converted <- problems %>%
  words_to_numbers %>%
  convert_format

remaining_problems <- converted[not_inches_or_cm(converted)]
pattern <- "^[4-7]\\s*'\\s*\\d+\\.?\\d*$"
index <- str_detect(remaining_problems, pattern)
remaining_problems[!index]
```
The *extract()* function permits using regex to extract information from strings into a table.
```{r}
s <- c("5'10", "6'1\"")
tab <- data.frame(x = s)
tab %>% extract(x, c("feet","inches"), regex = "(\\d)'(\\d{1,2})")
```
Let's add an extra expression to add a column for decimal inches when available.
```{r}
s <- c("5'10", "6'1\"", "5'8inches", "5'7.5")
tab <- data.frame(x = s)
extract(data = tab, col = x, into = c("feet", "inches", "decimal"), regex = "(\\d)'(\\d{1,2})(\\.\\d+)?")
```

We can now wrangle our data to recover as many heights as possible. 

Start by wrangling the height column to x'y format.

Add an original heights column so we can compare before and after.
```{r}
pattern <- "^([4-7])\\s*'\\s*(\\d+\\.?\\d*)$"
smallest <- 50
tallest <- 84

new_heights <- reported_heights %>%
  mutate(original = height,
         height = words_to_numbers(height) %>%
           convert_format()) %>%
  extract(height, c("feet", "inches"), regex = pattern, remove = FALSE) %>%
  mutate_at(c("height", "feet", "inches"), as.numeric) %>%
  mutate(guess = 12*feet + inches) %>%
  mutate(height = case_when(
    !is.na(height) & between(height, smallest, tallest) ~ height, # inches
    !is.na(height) & between(height/2.54, smallest, tallest) ~ height/2.54, # centimeters
    !is.na(height) & between(height*100/2.54, smallest, tallest) ~ height*100/2.54, # meters
    !is.na(guess) & between(guess, smallest, tallest) ~ guess, # feet'inches
    TRUE ~ as.numeric(NA))) %>%
  select(-guess)
```
Check the converted entries:
```{r}
new_heights %>%
  filter(not_inches(original)) %>%
  select(original, height) %>%
  arrange(height) %>%
  View()
```

Now, for a different wrangle: string splitting.
Say we read in a file without access to read_csv like this:
```{r}
filename <- system.file("extdata/murders.csv", package = "dslabs")
lines = readLines(filename)
lines %>% head()
```
*str_split()* splits strings on a provided delimiter.
```{r}
x <- str_split(lines, ",")
x %>% head()
```
The *map()* function from *purrr* applies the same function to each element in a list.

If we want to extract the first entry of each element in x, we can use *map()*.
```{r}
map(x, function(y) y[1]) %>% head()
```
Because list indexing is so common, *map()* infers this operation is wanted if provided an integer as the second argument.
```{r}
map(x, 1) %>% head()
```
*map_chr()* returns a character vector instead of a list. There are type-true map functions for all core types.
```{r}
map_chr(x,1) %>% head()
```
To create our murders data frame, we can use this code:
```{r}
col_names <- c("state", "abb", "region", "population", "total")
dat <- data.frame(map_chr(x,1),
                  map_chr(x,2),
                  map_chr(x,3),
                  map_chr(x,4),
                  map_chr(x,5)) %>%
  mutate_all(parse_guess) %>%
  setNames(col_names)
dat %>% head()
```
Using other functions in the *purrr* package, we can accomplish what we just did with much more efficient code.
```{r}
dat <- x %>%
  transpose() %>%
  map( ~ parse_guess(unlist(.))) %>%
  setNames(col_names) %>%
  as.data.frame()
dat %>% head()
```
There's another simplification we can do within *str_split()*. The option *simplifly = TRUE* makes the function return a matrix instead of a list.
```{r}
x <- str_split(lines, ",", simplify = TRUE)
col_names <- x[1,]
x <- x[-1,]
x %>% as_data_frame() %>%
  setNames(col_names) %>%
  mutate_all(parse_guess)
```
Practice with string splitting.
```{r}
monday <- "Mandy, Chris and Laura"
tuesday <- "Steve, Ruth and Frank"
staff <- c(monday, tuesday)
schedule <- data.frame(day = c("Monday", "Tuesday"), staff = staff, stringsAsFactors = FALSE)
schedule
```
```{r}
str_split(schedule$staff, ",\\s|\\sand\\s")
str_split(schedule$staff, ", | and ")
```
Either expression can be used with *unnest()* to create a tidy data table.
```{r}
tidy <- schedule %>%
  mutate(staff = str_split(schedule$staff, ", | and ")) %>%
  unnest()
tidy
```
We can also extract data from a PDF with *pdftools*.

One of the datasets provided in *dslabs* shows scientific funding rates by gender in the Netherlands:
```{r}
library(dslabs)
data("research_funding_rates")
research_funding_rates
```
The data come from a PNAS paper. However, the data are not provided in a spreadsheet; they are in a table in a PDF document. We could extract the numbers by hand, but this could lead to human error. Instead we can wrangle the data using R. 

We download the PDF document then import it into R using the following code:
```{r}
library("pdftools")
temp_file <- tempfile()
url <- "http://www.pnas.org/content/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf"
download.file(url, temp_file)
txt <- pdf_text(temp_file)
file.remove(temp_file)
```
If we examine the object text we notice that it is a character vector with an entry for each page. So we keep the page we want using the following code:
```{r}
raw_data_research_funding_rates <- txt[2]
raw_data_research_funding_rates %>% head
```
We see it is a long string. Each line on the page, including the table rows, is separated by the newline symbol *\n*.

Oddly, my version doesn't seem to have extracted the full numerical information. I'm in luck because the raw data has been included in *dslabs
```{r}
data("raw_data_research_funding_rates")
raw_data_research_funding_rates
```

We can create a list with lines of text as elements:
```{r}
tab <- str_split(raw_data_research_funding_rates, "\n")
tab <- tab[[1]]
tab %>% head
```
Info for column names is in entries three and four:
```{r}
the_names_1 <- tab[3]
the_names_2 <- tab[4]
```
Let's start by extracting table data from the first line:
```{r}
the_names_1
```
Let's remove leading space and everything following the comma. Then we can obtain the elements by splitting using the space. We will split only when there are two or more spaces to avoid splitting "Success rates" using `r \\s{2,}`:
```{r}
the_names_1 <- the_names_1 %>%
  str_trim() %>%
  str_replace_all(",\\s.", "") %>%
  str_split("\\s{2,}", simplify = TRUE)
the_names_1
```
Success rate = 100%! Now let's look at the second line.
```{r}
the_names_2
```
Now we can trim the leading space and then split on one or more spaces.
```{r}
the_names_2 <- the_names_2 %>%
  str_trim() %>%
  str_split("\\s+", simplify = TRUE)
the_names_2
```
There are now two character matrices we can join to generate one name for each column.
```{r}
tmp_names <- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = "_")
the_names <- c(the_names_2[1], tmp_names) %>%
  str_to_lower() %>%
  str_replace_all("\\s", "_")
the_names
```
We are now ready to get to the actual data. Examining `r tab`, we notice that the info is in lines 6 through 14. We can access it via `r str_split()`.
```{r}
new_research_funding_rates <- tab[6:14] %>%
  str_trim %>%
  str_split("\\s{2,}", simplify = TRUE) %>%
  data.frame(stringsAsFactors = FALSE) %>%
  setNames(the_names) %>%
  mutate_at(-1, parse_number)
new_research_funding_rates %>% head()
```
```{r}
identical(research_funding_rates, new_research_funding_rates)
```
It is often useful to `r recode()` names of variables when they are long either for ease of use or clear display in graphs.
```{r}
data("gapminder")
gapminder %>%
  filter(region == "Caribbean") %>%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()
```
Much of the space is wasted to accommodate long country names. Here are country names 12+ long.
```{r}
gapminder %>%
  filter(region == "Caribbean") %>%
  filter(str_length(country) >= 12) %>%
  distinct(country)
```
`r recode` lets us easily rename all instances of these countries.
```{r}
gapminder %>%
  filter(region == "Caribbean") %>%
  mutate(country = recode(country,
                          "Antigua and Barbuda" = "Antigua",
                          "Dominican Republic" = "DR",
                          "St. Vincent and the Grenadines" = "St. Vincent",
                          "Trinidad and Tobago" = "Trinidad")) %>%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()
```
There are other similar functions in the tidyverse including `r recode_factor()` and `r fct_recoder()`in the *forcats* function in the *tidyverse* package.
```{r}
gapminder %>% filter(region == "Middle Africa")
```
